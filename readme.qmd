---
title: "Analysis of Velib bike sharing"
date-modified: today
mermaid-format: png
format: 
  gfm:
    output-file: "README"
    output-ext: md
execute: 
  echo: false 
  warning: false
---


# Project Overview: Analyzing Velib Utilization Patterns in Paris

This project analyses the dynamics of the Parisian Velib bike sharing system. I use clustering, spatial and temporal visulization to uncover patterns in the flow of bikes in the network separately of weekdays and weekends. Both timeperiods exhibit quite different dynamics.

# The Dataset

The dataset consists of two tables, one containing general information (id, name, location etc.) for all station and another containing the number of bikes available at a given station at a given timepoint.

The data were acquired from 01/07/2024 to 11/02/2025 using the `General Bikeshare Feed` of the Velib system (available at: [https://www.velib-metropole.fr/donnees-open-data-gbfs-du-service-velib-metropole](https://www.velib-metropole.fr/donnees-open-data-gbfs-du-service-velib-metropole)). The feed was accessed using the [`gbfs-client`](https://github.com/jakehadar/bikeshare-client-python) package for python. The feed was queried at a time interval of 5 minutes.

After unnesting, the newly acquired data was pushed to a local `sqlite` database.

The original script is available at [./archive/server.py](./archive/server.py).

Data acquisition stopped due to a schema change in the feed. A new implementation using `dlt` is pending.

The timeseries data set contains 87 047 062 rows.

# Tools

- data load tool `dlt`: data ingestion (local `sqlite` database to AWS `Athena`, recurrent querying of the GBFS feed for future data acquisition)
- data build tool `dbt`: data transformation
- AWS `Quicksight`: dashboarding
- python libraries for data processing: `pandas`, `scikit-learn`


# Data pipeline schema

![](./img/mermaid-diagram-2025-04-27-214223.png)

# Data transformation

Data were analysed over the complete acquisition period. For each time point and station the relative occupation of the station was calculated by dividing the number of available bikes (`num_bikes_available`) by the sum of available bikes and available docks (`num_docks_availabe`) ([see also the dbt models](./dbt/athena_/models/)).

The relative occupation was then averaged for each station by the hour of the day ([see for example](./analysis/paris/rel_avail_weekdays/rel_avail_data.py)).
Separate datasets were created for weekdays and the weekend.

# Results

```{python}
#| echo: false
import sys
import os
import pandas as pd
import geopandas
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium.plugins import HeatMap
import contextily as ctx
import numpy as np
import matplotlib.gridspec as gridspec
```

## Network dynamics during weekdays

```{python}

# Load the saved data
wcss_df = pd.read_csv('./analysis/paris/rel_avail_weekdays/wcss_results.csv')
cluster_profiles_df = pd.read_csv('./analysis/paris/rel_avail_weekdays/cluster_profiles.csv', index_col=0)
cluster_sizes_df = pd.read_csv('./analysis/paris/rel_avail_weekdays/cluster_sizes.csv', index_col=0)
stations_clusters_with_info_df = pd.read_csv('./analysis/paris/rel_avail_weekdays/station_clusters_with_info.csv')
stations_clusters_with_info_df = geopandas.GeoDataFrame(
    stations_clusters_with_info_df, geometry=geopandas.points_from_xy(
        stations_clusters_with_info_df['lon'], stations_clusters_with_info_df['lat']
    ), crs = "EPSG:4326"
)
# stations_clusters_with_info_df.set_crs(2154)
```

### Code

```{python}
n_clusters = cluster_profiles_df.shape[0] # Get n_clusters from the loaded data
fontsize = 16 # Adjust as necessary to keep relative size between elements
# Create a gridspec layout
fig = plt.figure(figsize=(15, 20))
gs = gridspec.GridSpec(8, 2) # 8 rows, 2 columns
# Panel 1: Elbow Method (Span 2x1)
ax1 = plt.subplot(gs[0:2, 0])  # Span rows 0 and 1, column 0
ax1.plot(wcss_df['n_clusters'], wcss_df['wcss'])
ax1.set_title('Elbow Method', fontsize=fontsize)
ax1.set_xlabel('Number of clusters', fontsize=fontsize)
ax1.set_ylabel('WCSS', fontsize=fontsize)
ax1.tick_params(axis='both', which='major', labelsize=fontsize-2)
ax1.tick_params(axis='both', which='minor', labelsize=fontsize-2)

# Panel 2: Cluster Profiles (Span 2x1)
ax2 = plt.subplot(gs[0:2, 1])  # Span rows 0 and 1, column 1
palette = sns.color_palette("hls", n_clusters)
for i in range(n_clusters):
    ax2.plot(cluster_profiles_df.iloc[i], label=f'Cluster {i+1}', color=palette[i])
ax2.set_title('Cluster Profiles (Average Availability per Hour)', fontsize=fontsize)
ax2.set_xlabel('Hour', fontsize=fontsize)
ax2.set_ylabel('Average Relative Availability', fontsize=fontsize)
ax2.set_xticklabels(range(24), fontsize=fontsize-2)
ax2.tick_params(axis='y', which='major', labelsize=fontsize-2)

ax2.legend(fontsize=fontsize-4)
ax2.grid(True)

# Panel 3: Cluster Sizes (Span 2x1)
ax3 = plt.subplot(gs[2:4, 0])  # Span rows 2 and 3, column 0
cluster_sizes_df.sort_index().plot(kind='bar', ax=ax3, legend=False)
ax3.set_title('Cluster Sizes', fontsize=fontsize)
ax3.set_xlabel('Cluster', fontsize=fontsize)
ax3.set_ylabel('Number of Stations', fontsize=fontsize)
ax3.tick_params(axis='both', which='major', labelsize=fontsize-2)
ax3.tick_params(axis='both', which='minor', labelsize=fontsize-2)


# Panel 4: Cluster Heatmap (Span 2x1)
ax4 = plt.subplot(gs[2:4, 1])  # Span rows 2 and 3, column 1
sns.heatmap(cluster_profiles_df, cmap="YlGnBu", ax=ax4)
ax4.set_title("Heatmap of Average Availability by Cluster", fontsize=fontsize)
ax4.set_xlabel('Hour', fontsize=fontsize)
ax4.set_ylabel("Cluster", fontsize=fontsize)
ax4.set_xticklabels(range(24), fontsize=fontsize-2)
ax4.set_yticklabels(ax4.get_yticklabels(), fontsize=fontsize-2, rotation=0)
# Panel 5: Geographic Distribution of Clusters (Span 4x4)
ax5 = plt.subplot(gs[4:8, :])  # Span rows 4, 5, 6 and 7, and both columns
# sns.scatterplot(
#     x='lon',
#     y='lat',
#     hue='cluster',
#     palette=sns.color_palette("hls", n_clusters),
#     data=stations_clusters_with_info_df,
#     ax=ax5,
#     s=20 # Adjust marker size
# )
gdf_webmercator = stations_clusters_with_info_df.to_crs(epsg=3857)
# First, create a dictionary to map cluster numbers to colors
cluster_colors = {i: color for i, color in enumerate(sns.color_palette("hls", n_colors=n_clusters))}

# Then, plot using the map
gdf_webmercator.plot(ax=ax5, c=gdf_webmercator['cluster'].subtract(1).map(cluster_colors))
ax5.set_title('Geographic Distribution of Clusters', fontsize=fontsize)
ax5.set_xlabel('Longitude', fontsize=fontsize)
ax5.set_ylabel('Latitude', fontsize=fontsize)
ax5.set_aspect('equal', adjustable='datalim') # Keep aspect ratio for map
ctx.add_basemap(ax5, crs =gdf_webmercator.crs.to_string(), source=ctx.providers.CartoDB.Positron)
ax5.set_xticks([])
ax5.set_yticks([])
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)
ax5.spines['left'].set_visible(False)
ax5.spines['bottom'].set_visible(False)
ax5.set_xlabel('')
ax5.set_ylabel('')
plt.tight_layout()  # Adjust layout to prevent overlapping subplots

```

### Interpretation

The number of clusters was identified using the elbow method, there is no clearcut results, but a choice of $k=5$ seems justified as it provided a meaningful distinction of temporal patterns and geographic concentrations that align with expected urban mobility behaviors, offering a coherent basis for analysis.

Given the five clusters, temporal analysis of the hourly mean relative occupancy over the course of a typical working day shows interesting dynamics. 

Cluster no. 1 is the biggest cluster with about 500 stations (corresponding to about 30% of all stations in the network). Its mean occupancy goes never beyond 30% and is very low (<20%) during working hours (8am to 20pm). The stations belonging to cluster 1, are mainly situated in the outer ring of the Parisian arrondissements on the Rive droite of the river Seine (Arr. 17, 18, 19, 20) and in the 13th and 14th arrondissement on the Rive Gauche.

The mean occupancy in cluster 2 is quite stable between 60-70% throughout the day. Stations assigned to cluster 2 are concentrated in two regions of Paris: 1) In the eastern part of the inner circle of arrondissements (3rd, 4th, 5th) and 2) In the outer south-western part, around and south of the Eiffel tower which might indicate 

The remaining clusters 3, 4, and 5 exhibit anti-correlated periodic patterns with pronounced changes in occupancy during communting hours (6-9am and 4-6pm). Stations in the 6th, 7th, and 8th arrondissement (cluster 4) are lowly occupied in the morning and experience a large increase in occupancy, consistent with commuter influx from clusters 3 and 5. 

One has to keep in mind, that only net fluxes can be observed in this dataset, *i.e.* the flux path cannot be inferred from these data meaning outflows from clusters 3 and 5 in the morning might directly go into cluster 4 while a flux equilibrium between clusters 3,5,2 and 4 an equally good explanation for the observed pattern. 

Keeping this in mind, the observed net flow patterns are still informative as to whether specific stations or clusters of stations exhibit certain characteristics and guide network operations: 

Stations or clusters that show a significant decrease in available bikes during morning peak hours, suggest a high volume of net rentals typical of commuter activity while stations or clusters that exhibit a strong increase in available bikes during evening hours, indicate a net inflow of bikes (more returns than rentals), potentially from users returning bikes after work or leisure.

Stations with consistent net decrease during peak usage times are candidates for having bikes sent during proactive rebalancing while bikes might need to be removed from stations that saturate.

## Network dynamics during weekends

### Code

```{python}

# Load the saved data
wcss_df = pd.read_csv('./analysis/paris/rel_avail_weekend/wcss_results.csv')
cluster_profiles_df = pd.read_csv('./analysis/paris/rel_avail_weekend/cluster_profiles.csv', index_col=0)
cluster_sizes_df = pd.read_csv('./analysis/paris/rel_avail_weekend/cluster_sizes.csv', index_col=0)
stations_clusters_with_info_df = pd.read_csv('./analysis/paris/rel_avail_weekend/station_clusters_with_info.csv')
stations_clusters_with_info_df = geopandas.GeoDataFrame(
    stations_clusters_with_info_df, geometry=geopandas.points_from_xy(
        stations_clusters_with_info_df['lon'], stations_clusters_with_info_df['lat']
    ), crs = "EPSG:4326"
)
# stations_clusters_with_info_df.set_crs(2154)
```

```{python}
n_clusters = cluster_profiles_df.shape[0] # Get n_clusters from the loaded data
fontsize = 16 # Adjust as necessary to keep relative size between elements
# Create a gridspec layout
fig = plt.figure(figsize=(15, 20))
gs = gridspec.GridSpec(8, 2) # 8 rows, 2 columns
# Panel 1: Elbow Method (Span 2x1)
ax1 = plt.subplot(gs[0:2, 0])  # Span rows 0 and 1, column 0
ax1.plot(wcss_df['n_clusters'], wcss_df['wcss'])
ax1.set_title('Elbow Method', fontsize=fontsize)
ax1.set_xlabel('Number of clusters', fontsize=fontsize)
ax1.set_ylabel('WCSS', fontsize=fontsize)
ax1.tick_params(axis='both', which='major', labelsize=fontsize-2)
ax1.tick_params(axis='both', which='minor', labelsize=fontsize-2)

# Panel 2: Cluster Profiles (Span 2x1)
ax2 = plt.subplot(gs[0:2, 1])  # Span rows 0 and 1, column 1
palette = sns.color_palette("hls", n_clusters)
for i in range(n_clusters):
    ax2.plot(cluster_profiles_df.iloc[i], label=f'Cluster {i+1}', color=palette[i])
ax2.set_title('Cluster Profiles (Average Availability per Hour)', fontsize=fontsize)
ax2.set_xlabel('Hour', fontsize=fontsize)
ax2.set_ylabel('Average Relative Availability', fontsize=fontsize)
ax2.set_xticklabels(range(24), fontsize=fontsize-2)
ax2.tick_params(axis='y', which='major', labelsize=fontsize-2)

ax2.legend(fontsize=fontsize-4)
ax2.grid(True)

# Panel 3: Cluster Sizes (Span 2x1)
ax3 = plt.subplot(gs[2:4, 0])  # Span rows 2 and 3, column 0
cluster_sizes_df.sort_index().plot(kind='bar', ax=ax3, legend=False)
ax3.set_title('Cluster Sizes', fontsize=fontsize)
ax3.set_xlabel('Cluster', fontsize=fontsize)
ax3.set_ylabel('Number of Stations', fontsize=fontsize)
ax3.tick_params(axis='both', which='major', labelsize=fontsize-2)
ax3.tick_params(axis='both', which='minor', labelsize=fontsize-2)


# Panel 4: Cluster Heatmap (Span 2x1)
ax4 = plt.subplot(gs[2:4, 1])  # Span rows 2 and 3, column 1
sns.heatmap(cluster_profiles_df, cmap="YlGnBu", ax=ax4)
ax4.set_title("Heatmap of Average Availability by Cluster", fontsize=fontsize)
ax4.set_xlabel('Hour', fontsize=fontsize)
ax4.set_ylabel("Cluster", fontsize=fontsize)
ax4.set_xticklabels(range(24), fontsize=fontsize-2)
ax4.set_yticklabels(ax4.get_yticklabels(), fontsize=fontsize-2, rotation=0)
# Panel 5: Geographic Distribution of Clusters (Span 4x4)
ax5 = plt.subplot(gs[4:8, :])  # Span rows 4, 5, 6 and 7, and both columns
# sns.scatterplot(
#     x='lon',
#     y='lat',
#     hue='cluster',
#     palette=sns.color_palette("hls", n_clusters),
#     data=stations_clusters_with_info_df,
#     ax=ax5,
#     s=20 # Adjust marker size
# )
gdf_webmercator = stations_clusters_with_info_df.to_crs(epsg=3857)
# First, create a dictionary to map cluster numbers to colors
cluster_colors = {i: color for i, color in enumerate(sns.color_palette("hls", n_colors=n_clusters))}

# Then, plot using the map
gdf_webmercator.plot(ax=ax5, c=gdf_webmercator['cluster'].subtract(1).map(cluster_colors))
ax5.set_title('Geographic Distribution of Clusters', fontsize=fontsize)
ax5.set_xlabel('Longitude', fontsize=fontsize)
ax5.set_ylabel('Latitude', fontsize=fontsize)
ax5.set_aspect('equal', adjustable='datalim') # Keep aspect ratio for map
ctx.add_basemap(ax5, crs =gdf_webmercator.crs.to_string(), source=ctx.providers.CartoDB.Positron)
ax5.set_xticks([])
ax5.set_yticks([])
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)
ax5.spines['left'].set_visible(False)
ax5.spines['bottom'].set_visible(False)
ax5.set_xlabel('')
ax5.set_ylabel('')
plt.tight_layout()  # Adjust layout to prevent overlapping subplots

```

### Interpretation

<!-- # Outlook

## Occupancy analysis
- per district/city
- correlation with sociodemographic indicators (salary, age, level of education, total population)
- correlation with station elevation
- stratified analysis mechanical/ebikes


## Network structure analysis (solely based on stations)

- theoretical capacity -->